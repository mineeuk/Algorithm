{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. BLUE\n",
    "2. ROUGE-L\n",
    "3. METEOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BLUE (Bilingual Evaluation Understudy)\n",
    "BLEU는 기계 번역(Machine Translation)의 품질을 평가하는 가장 널리 사용되는 지표 중 하나로, 예측된 번역(Hypothesis Translation, HYP)이 참조 번역(Reference Translation, REF)과 얼마나 유사한지 n-그램(n-gram) 기반으로 평가합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU 점수는 아래의 요소들을 사용하여 계산됩니다:\n",
    "\n",
    "a. n-그램 정밀도 (n-gram Precision) 예측된 번역(HYP)에서 참조 번역(REF)과 일치하는 n-그램의 비율. 단순히 일치하는 n-그램의 개수를 세는 방식.\n",
    "\n",
    "b. 클립된 정밀도 (Clipped Precision) n-그램이 중복될 경우, 참조 번역에서 해당 n-그램의 최대 빈도를 초과한 부분을 제거. 예: REF에 \"the cat is on the mat\"가 있고 HYP에 \"the the the the the\"가 있다면, \"the\"의 빈도는 REF의 최대 빈도(2회)로 제한.\n",
    "\n",
    "c. 짧은 번역에 대한 페널티 (Brevity Penalty, BP) HYP가 지나치게 짧아지는 것을 방지하기 위해 추가된 요소. HYP의 길이가 REF보다 짧으면 페널티를 적용.\n",
    "\n",
    "BLEU의 최종 계산 공식은 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = BP \\cdot \\exp \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\log p_n \\right)\n",
    "$$\n",
    "\n",
    "BLEU 점수의 주요 요소는 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "p_n: \\text{클립된 } n\\text{-그램 정밀도.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "BP: \\text{Brevity Penalty.}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "**1. 의미적 일치 부족**:BLEU는 단순히 n-그램 기반 정밀도를 계산하므로, 문맥적 의미나 의미적 유사성을 평가하지 못합니다.\n",
    "동의어 사용이나 문장의 구조적 변형(paraphrasing)을 제대로 반영하지 못합니다.\n",
    "ex) \"The car is fast\"와 \"The automobile is quick\"은 의미적으로 동일하지만, BLEU는 낮은 점수를 부여할 수 있습니다.\\\n",
    "**2. 복수 참조 번역 문제**:BLEU는 다중 참조 번역(multiple reference translations)을 허용하지만, 참조 번역이 부족하거나 표현이 제한적일 경우 점수가 왜곡될 수 있습니다.\n",
    "다양한 번역 표현이 존재하지만, 참조 번역에 포함되지 않았다면 BLEU 점수가 낮게 평가될 수 있습니다.\\\n",
    "**3. 짧은 번역과 긴 번역에 대한 민감성**: 짧은 번역: Brevity Penalty(BP)로 일부 보완되지만, 여전히 짧은 번역이 길이에 따른 불공정한 이점을 가질 수 있습니다.\n",
    "긴 번역: 불필요한 단어를 추가하거나 중복된 n-그램이 많은 긴 번역은 점수를 부풀릴 가능성이 있습니다.\\\n",
    "**4. 인간 평가와의 낮은 상관관계** : BLEU는 n-그램의 단순한 정밀도를 기반으로 하므로, 문장 유창성(fluency)이나 의미 전달의 정확성(adequacy)과 같은 인간 평가 기준과 상관관계가 낮을 수 있습니다. 특정 언어나 문체에서 BLEU의 한계가 더 두드러질 수 있습니다.\\\n",
    "**5. 번역 길이와 구조의 민감성** : BLEU는 번역의 전체 구조를 고려하지 않습니다. 문장의 순서나 구문이 다르더라도 같은 점수를 받을 수 있습니다.\n",
    "ex)\"The cat sat on the mat\"와 \"On the mat, the cat sat\"은 점수가 같을 수 있지만, 구조적 차이가 무시됩니다.\\\n",
    "**6. 고정된 n-그램 기반 평가** : 일반적으로 BLEU는 1~4-그램 정밀도를 사용합니다.\n",
    "너무 작은 n-그램(예: 1-그램)은 전체 문맥을 무시하고, 너무 큰 n-그램(예: 5-그램 이상)은 데이터 희소성(sparsity) 문제를 초래할 수 있습니다. 복잡한 문장은 n-그램 점수만으로는 정확히 평가하기 어렵습니다.\\\n",
    "**7. 의미적 동의어와 표현 다양성 무시** : \n",
    "BLEU는 n-그램 일치를 중시하기 때문에, 동의어나 표현 방식의 다양성을 충분히 반영하지 못합니다.\n",
    "ex) \"good\"와 \"great\"는 문맥상 유사하지만 BLEU는 이를 일치하지 않는 것으로 간주합니다.\\\n",
    "**8. 초점이 정밀도에 치우침** : \n",
    "BLEU는 정밀도(Precision) 중심의 지표로, **재현율(Recall)**은 무시합니다.\n",
    "즉, 모든 참조 번역의 정보를 포괄하지 못할 가능성이 있습니다.\\\n",
    "**9. 도메인 특화 문제** : \n",
    "BLEU는 특정 도메인이나 언어에서 더 큰 제약을 가질 수 있습니다.\n",
    "ex) 어순이 중요한 언어(한국어, 일본어)나 구조적으로 복잡한 언어에서는 BLEU 점수가 부정확할 가능성이 있습니다.\\\n",
    "**10. 평가의 불안정성** : \n",
    "BLEU는 번역 길이, 참조 번역의 수, 데이터의 품질에 따라 점수가 크게 영향을 받을 수 있습니다.\n",
    "데이터가 불균형하거나 참조 번역의 질이 낮을 경우, BLEU 점수의 신뢰도가 떨어집니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보완하려면?\n",
    "**1.다른 평가 지표와 함께 사용** :\n",
    "BERTScore, COMET, ROUGE와 같은 지표를 BLEU와 함께 사용하면 평가의 신뢰도를 높일 수 있습니다.\\\n",
    "**2.인간 평가 병행** :\n",
    "BLEU의 한계를 보완하기 위해 인간 평가와 결합하여 결과를 해석합니다.\\\n",
    "**3.복수 참조 번역** :\n",
    "참조 번역을 다양하게 제공하여 표현의 폭을 넓히고 BLEU 점수의 왜곡을 줄입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- **Paper**: [BLEU: a Method for Automatic Evaluation of Machine Translation (ACL 2002)](https://aclanthology.org/P02-1040.pdf)\n",
    "- **GitHub**: [sacrebleu](https://github.com/mjpost/sacrebleu)\n",
    "- **HuggingFace**: [evaluate-metric/bleu](https://huggingface.co/spaces/evaluate-metric/bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Examples for BLUE\n",
    "\n",
    "### Inputs\n",
    "- `predictions` (list of str): 예측된 번역(HYP)의 문자열 목록.\n",
    "- `references` (list of list of str): 하나 이상의 참조 번역(REF) 목록.\n",
    "\n",
    "### Outputs\n",
    "- `score` (float): BLEU 점수.\n",
    "- `precisions` (list of floats): 1-그램부터 4-그램까지의 정밀도.\n",
    "- `brevity_penalty` (float): Brevity Penalty 값.\n",
    "- `length_ratio` (float): HYP와 REF의 길이 비율.\n",
    "- `translation_length` (int): 예측된 번역(HYP)의 총 길이.\n",
    "- `reference_length` (int): 참조 번역(REF)의 총 길이.\n",
    "- 출력값은 다음과 같은 형식으로 반환됩니다: \n",
    "{\n",
    "    \"<span style='color:red'>score</span>\": <span style='color:green'>45.67</span>,\n",
    "    \"<span style='color:red'>precisions</span>\": [<span style='color:green'>50.0</span>, <span style='color:green'>40.0</span>, <span style='color:green'>30.0</span>, <span style='color:green'>20.0</span>],\n",
    "    \"<span style='color:red'>brevity_penalty</span>\": <span style='color:green'>0.8</span>,\n",
    "    \"<span style='color:red'>length_ratio</span>\": <span style='color:green'>0.9</span>,\n",
    "    \"<span style='color:red'>translation_length</span>\": <span style='color:green'>90</span>,\n",
    "    \"<span style='color:red'>reference_length</span>\": <span style='color:green'>100</span>\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sacrebleu 라이브러리를 이용한 BLEU 평가\n",
    "### 주요 옵션:\n",
    "- `smooth-method`(str): 점수 계산 시 부드럽게 처리하기 위한 방법 지정 (Default: \"- `exp`\").\\\n",
    "- `lowercase`(boolean): True인 경우 소문자로 변환하여 평가 (Default: - `False`).\\\n",
    "- `tokenize` (str): 토큰화 방법 지정. \"intl\", \"zh\", \"13a\", \"none\" 등이 가능 (Default: \"- `13a`\").\\\n",
    "- `force` (boolean): 토큰화 강제 적용 (Default: - `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 예제 번역 및 참조 번역\n",
    "hypothesis = [\"The cat is on the mat.\"]\n",
    "reference = [[\"The cat is sitting on the mat.\"]]\n",
    "\n",
    "# BLEU 점수 계산\n",
    "bleu_score = sacrebleu.corpus_bleu(hypothesis, reference)\n",
    "\n",
    "# BLEU 점수 출력\n",
    "print(\"BLEU Score:\", bleu_score.score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace의 evaluate 라이브러리를 이용한 BLEU 평가\n",
    "### 주요 옵션:\n",
    "- `max_order` (int): 최대 n-그램 길이 설정 (Default: - `4` ).\n",
    "- `smooth`  (boolean): 점수 부드럽게 처리 여부 (Default: - `False` ).\n",
    "- `lowercase` (boolean): True인 경우 모든 텍스트를 소문자로 변환하여 비교 (Default: - `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate-metric 라이브러리 설치\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# HYP와 REF 정의\n",
    "hypothesis = [\"The cat is on the mat.\"]\n",
    "references = [[\"The cat is sitting on the mat.\"]]\n",
    "\n",
    "# 라이브러리에서 지원하는 여러 평가 지표 중 BLEU 불러오기\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# BLEU 점수 계산\n",
    "bleu_score = bleu.compute(predictions=hypothesis, references=references)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"BLEU Score:\", bleu_score[\"bleu\"])\n",
    "print(\"Precisions:\", bleu_score[\"precisions\"]) #1-그램부터 4-그램까지의 정밀도.\n",
    "print(\"Brevity Penalty:\", bleu_score[\"brevity_penalty\"]) \n",
    "print(\"Length Ratio:\", bleu_score[\"length_ratio\"]) #번역 길이와 참고 길이의 비율.\n",
    "print(\"Translation Length:\", bleu_score[\"translation_length\"]) #번역된 텍스트 (HYP)의 총 길이.\n",
    "print(\"Reference Length:\", bleu_score[\"reference_length\"]) #참조 번역 (REF)의 총 길이."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L\n",
    "\n",
    "## 1. ROUGE-L(Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- 기계 번역, 요약 등 자연어 처리 작업에서 텍스트 간 유사성을 평가하기 위해 사용되는 지표입니다. \n",
    "- **ROUGE-L**은 두 텍스트 간의 가장 긴 공통 부분 문자열(LCS, Longest Common Subsequence)을 기반으로 계산됩니다.\n",
    "  - LCS를 활용함으로써 단어 순서가 중요한 텍스트 평가에 적합.\n",
    "  - Precision, Recall, F1-Score로 결과를 제공.\n",
    "\n",
    "## Limitations\n",
    "1. **문맥적 의미 무시** : ROUGE는 단어 수준의 일치를 평가하기 때문에 문맥적 의미나 동의어를 반영하지 못함. ex) \"The car is fast\"와 \"The automobile is quick\"는 의미적으로 동일하지만 낮은 점수를 받을 수 있음.\\\n",
    "2. **문장 길이에 민감** : 짧은 텍스트는 길이 차이로 인해 낮은 점수를 받을 가능성이 있음. 긴 문장은 공통 부분이 많아 점수가 왜곡될 수 있음.\\\n",
    "3. **어휘 수준의 평가 한계** : 단어의 순서와 일치 여부를 중시하지만, 텍스트의 유창성이나 의미 전달의 정확성을 평가하지 못함.\\\n",
    "4. **어휘 다양성 부족 문제** : 동의어나 문장 변형(paraphrasing)에 대한 반영이 어려움.\n",
    "\n",
    "## 보완하려면?\n",
    "1. **다른 평가 지표와 함께 사용** : BLEU, BERTScore, COMET과 같은 지표를 병행하여 텍스트 평가의 신뢰성을 높임.\n",
    "2. **인간 평가와 결합** : 인간 평가자의 피드백을 통해 ROUGE-L의 단점을 보완.\n",
    "3. **전처리 강화** : 스테머(stemmer)와 lemmatizer를 활용해 어휘 차이를 줄이고 동의어를 반영.\n",
    "\n",
    "## References\n",
    "- **Paper**: [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013.pdf)\n",
    "- **GitHub**: [Google's ROUGE Scorer](https://github.com/google-research/google-research/tree/master/rouge)\n",
    "- **HuggingFace**: [evaluate-metric/rouge](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Examples for ROUGE-L\n",
    "\n",
    "### Inputs\n",
    "- `predictions` (list of str): 모델이 예측한 번역의 문자열 목록.\n",
    "- `references` (list of str): 참조 번역의 문자열 목록.\n",
    "\n",
    "### Outputs\n",
    "- `rouge1` (dict): ROUGE-1의 Precision, Recall, F1 점수.\n",
    "- `rougeL` (dict): ROUGE-L의 Precision, Recall, F1 점수.\n",
    "- 출력값은 다음과 같은 형식으로 반환됩니다:\n",
    "{\n",
    "    '<span style=\"color:red;\">rouge1</span>': {\n",
    "        '<span style=\"color:red;\">precision</span>': <span style=\"color:green;\">0.75</span>,\n",
    "        '<span style=\"color:red;\">recall</span>': <span style=\"color:green;\">0.6</span>,\n",
    "        '<span style=\"color:red;\">fmeasure</span>': <span style=\"color:green;\">0.6667</span>\n",
    "    },\n",
    "    '<span style=\"color:red;\">rougeL</span>': {\n",
    "        '<span style=\"color:red;\">precision</span>': <span style=\"color:green;\">0.7</span>,\n",
    "        '<span style=\"color:red;\">recall</span>': <span style=\"color:green;\">0.5833</span>,\n",
    "        '<span style=\"color:red;\">fmeasure</span>': <span style=\"color:green;\">0.6364</span>\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 예제 번역 및 참조 번역\n",
    "hypothesis = \"The cat is on the mat.\"\n",
    "reference = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# ROUGE 점수 계산\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, hypothesis)\n",
    "print(\"ROUGE-1 Score:\", scores['rouge1'].fmeasure)\n",
    "print(\"ROUGE-L Score:\", scores['rougeL'].fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace의 evaluate 라이브러리를 이용한 ROUGE-L 평가\n",
    "### **옵션 설명**\n",
    "- `use_aggregator` (boolean): True일 경우 여러 샘플의 평균 값을 사용 (Default: - `True`).\n",
    "- `use_stemmer` (boolean): True일 경우 어간 추출을 사용하여 어휘 변형을 최소화 (Default: - `False`).\n",
    "- `rouge_types` (list of str): 사용할 ROUGE 타입 지정 (예: ['rouge1', 'rougeL']).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# ROUGE 라이브러리 로드\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# 예제 번역 및 참조 번역\n",
    "predictions = [\"The cat is on the mat.\"]\n",
    "references = [\"The cat is sitting on the mat.\"]\n",
    "\n",
    "# ROUGE 점수 계산\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"ROUGE-1 Score:\", results[\"rouge1\"])\n",
    "print(\"ROUGE-L Score:\", results[\"rougeL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "- **METEOR**는 기계 번역 품질 평가 지표 중 하나로, 정확성(Precision)과 재현율(Recall)을 모두 고려한 점수를 제공합니다.\n",
    "- 단어 수준의 매칭 외에도 **동의어**, **어간(stemming)**, **어휘 유사성** 등을 반영하여 BLEU의 단점을 보완.\n",
    "- Precision과 Recall의 가중치를 사용하여 F1 점수를 계산하며, 추가로 단어 순서를 고려하여 패널티를 적용.\n",
    "\n",
    "### Limitations\n",
    "1. **문맥 이해 부족** : METEOR는 단어 및 어휘 수준의 매칭에 초점을 맞추기 때문에 문맥적 의미를 완전히 반영하지 못함.\n",
    "2. **느린 계산 속도** : BLEU 등 n-그램 기반 지표보다 계산량이 많아 속도가 느릴 수 있음.\n",
    "3. **도메인 의존성** : 특정 언어나 도메인에 특화되지 않으면 동의어 및 어휘 유사성 매칭에서 제한적일 수 있음.\n",
    "4. **참조 번역 의존성**: 고품질 참조 번역이 없을 경우 METEOR 점수가 왜곡될 수 있음.\n",
    "\n",
    "### 보완하려면?\n",
    "1. **다른 지표와 함께 사용** : BLEU, ROUGE, BERTScore 등과 함께 사용하여 평가 신뢰도를 높임.\n",
    "2. **데이터 전처리 강화** : 어간 추출 및 동의어 매칭 사전을 개선하여 어휘 유사성 평가 정확도를 향상.\n",
    "3. **도메인 특화 모델 활용** : 도메인에 적합한 사전 학습된 언어 모델과 결합하여 평가 품질을 보완.\n",
    "\n",
    "### References\n",
    "- **Paper**: [METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments](https://aclanthology.org/W05-0909/)\n",
    "- **NLTK Documentation**: [NLTK METEOR Score](https://www.nltk.org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Examples for METOR\n",
    "### Inputs\n",
    "- `reference` (str): 참조 번역.\n",
    "- `hypothesis` (str): 모델이 생성한 예측 번역.\n",
    "\n",
    "### Outputs\n",
    "- `score` (float): METEOR 점수.\n",
    "- 출력값은 다음과 같은 형식으로 반환됩니다:\n",
    "{\n",
    "    '<span style=\"color:red;\">score</span>': <span style=\"color:green;\">0.8571</span>\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit) 라이브러리\n",
    "- 자연어 처리(NLP) 작업을 수행하기 위해 개발된 파이썬 라이브러리로, 교육 및 연구 목적으로 설계되었습니다.\n",
    "- NLTK는 텍스트 전처리, 토큰화, 품사 태깅, 문장 파싱, 언어 모델링 등 다양한 NLP 작업을 지원합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 라이브러리를 이용한 METEOR 평가\n",
    "**METOR는 NLTK 라이브러리를 활용해 직접 계산해야 합니다.**\n",
    "- METEOR 점수를 직접 계산한 뒤, BLEU, ROUGE 등 다른 지표와 병합해 보고서를 작성할 수 있습니다.\n",
    "- HuggingFace나 sacreBLEU에서 METEOR을 공식 지원하지 않으므로, NLTK의 meteor_score가 가장 적합한 선택입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# 예제 번역 및 참조 번역\n",
    "hypothesis = [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n",
    "reference = [\"The\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# NLTK 설치 및 다운로드\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# METEOR 점수 계산\n",
    "meteor_score = nltk.translate.meteor_score.single_meteor_score(reference, hypothesis)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"METEOR Score:\", meteor_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error 해결\n",
    "- 토큰화: hypothesis와 reference를 단어 리스트로 변환해야 합니다.\n",
    "예: \"The cat is on the mat.\" → [\"The\", \"cat\", \"is\", \"on\", \"the\", \"mat\"].\n",
    "- NLTK의 요구사항:single_meteor_score 함수는 문자열이 아닌 토큰화된 단어 리스트를 입력으로 받습니다.\n",
    "\n",
    "### 참고\n",
    "만약 여러 문장을 한 번에 처리하려면 - `meteor_score.meteor_score` 함수와 다중 참조를 사용할 수도 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
